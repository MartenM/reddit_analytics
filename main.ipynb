{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import praw\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math as m\n",
    "\n",
    "from utility import log\n",
    "\n",
    "from networkx.algorithms import approximation\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Before starting with the actual algorithms we need to apply some prepocessing. For example removing data that is not relevant for our research, and extracting all nodes and links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:22:35] Loading data\n"
     ]
    }
   ],
   "source": [
    "# Load the datafile\n",
    "log(\"Loading data\")\n",
    "file_path = \"data/soc-redditHyperlinks-body.tsv\"\n",
    "raw_data = pd.read_csv(file_path, sep=\"\\t\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:22:42] Dropping\n",
      "[13:22:42] Creating list of all nodes\n",
      "[13:22:42] Nodes: 35,776\n",
      "[13:22:42] Links: 286,561\n"
     ]
    }
   ],
   "source": [
    "# Remove columns we won't be using\n",
    "log(\"Dropping\")\n",
    "data = raw_data.drop(columns=['POST_ID', 'PROPERTIES'])\n",
    "\n",
    "# Create a list of all nodes\n",
    "log(\"Creating list of all nodes\")\n",
    "nodes = pd.concat([data['SOURCE_SUBREDDIT'], data['TARGET_SUBREDDIT']], axis=0)\n",
    "nodes.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "log(f\"Nodes: {nodes.size:,}\")\n",
    "log(f\"Links: {data.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:22:45] Compressed links: 137,821\n"
     ]
    }
   ],
   "source": [
    "# Reduce the amount of links to just one direct link.\n",
    "# This can be useful if we want a more compressed view of the data.\n",
    "links_compressed = data.drop(columns=[\"TIMESTAMP\"]).groupby(['SOURCE_SUBREDDIT', 'TARGET_SUBREDDIT']).agg('sum')\n",
    "\n",
    "log(f\"Compressed links: {links_compressed.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:22:48] Preparing nx.MultiDiGraph()...\n",
      "[13:22:48] Adding nodes...\n",
      "[13:22:48] Adding edges...\n"
     ]
    }
   ],
   "source": [
    "log(\"Preparing nx.MultiDiGraph()...\")\n",
    "mGraph = nx.MultiDiGraph()\n",
    "\n",
    "log(\"Adding nodes...\")\n",
    "for node in nodes:\n",
    "    mGraph.add_node(node)\n",
    "\n",
    "log(\"Adding edges...\")\n",
    "for index, edge in data.iterrows():\n",
    "    mGraph.add_edge(edge['SOURCE_SUBREDDIT'], edge['TARGET_SUBREDDIT'], sentiment=edge['LINK_SENTIMENT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anaylsis\n",
    "This is the analysis of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:22:44] mGraph > Vertices: \t35,776\n",
      "[09:22:44] mGraph > Edges: \t286,561\n",
      "[09:22:44] In degree: (Most referenced subreddits)\n",
      "               SOURCE  degree\n",
      "34          askreddit    7329\n",
      "22               iama    3694\n",
      "7741             pics    2779\n",
      "42     writingprompts    2490\n",
      "11294          videos    2446\n",
      "[09:22:44] Out degree: (Referencing other subreddits)\n",
      "               SOURCE  degree\n",
      "70     subredditdrama    4665\n",
      "16        circlebroke    2358\n",
      "6400  shitliberalssay    1968\n",
      "126      outoftheloop    1958\n",
      "79          copypasta    1824\n"
     ]
    }
   ],
   "source": [
    "log(f\"mGraph > Vertices: \\t{mGraph.number_of_nodes():,}\")\n",
    "log(f\"mGraph > Edges: \\t{mGraph.number_of_edges():,}\")\n",
    "\n",
    "in_degree = pd.DataFrame(mGraph.in_degree(), columns=['SOURCE', 'degree'])\n",
    "out_degree = pd.DataFrame(mGraph.out_degree(), columns=['SOURCE', 'degree'])\n",
    "\n",
    "in_degree.sort_values('degree', ascending=False, inplace=True)\n",
    "out_degree.sort_values('degree', ascending=False, inplace=True)\n",
    "\n",
    "log(\"In degree: (Most referenced subreddits)\")\n",
    "print(in_degree.head(5))\n",
    "\n",
    "log(\"Out degree: (Referencing other subreddits)\")\n",
    "print(out_degree.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation of centrality indices: Degree Centrality, Betweenness Centrality, Closeness Centrality and Eigenvector Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:55:26] Total Degree-centrality:\n",
      "           SOURCE  degree_in  degree_out  total_degree_centrality\n",
      "0       askreddit       7329        1338                     8667\n",
      "1            iama       3694        1181                     4875\n",
      "2            pics       2779           4                     2783\n",
      "3  writingprompts       2490        1707                     4197\n",
      "4          videos       2446           4                     2450\n",
      "[10:55:37] Betweenness Centrality: (Influence of node in graph)\n",
      "             SOURCE  betweenness_centrality\n",
      "34        askreddit                0.059975\n",
      "22             iama                0.057724\n",
      "70   subredditdrama                0.049607\n",
      "42   writingprompts                0.034253\n",
      "126    outoftheloop                0.020895\n",
      "[10:55:39] Top 5 Nodes by Eigenvector Centrality:\n",
      "                   SOURCE  eigenvector_centrality\n",
      "70         subredditdrama                0.284837\n",
      "626                 drama                0.181029\n",
      "79              copypasta                0.163165\n",
      "31    circlejerkcopypasta                0.160993\n",
      "6400      shitliberalssay                0.151531\n",
      "[10:56:12] Top 5 Nodes by Closeness Centrality:\n",
      "           SOURCE  closeness_centrality\n",
      "0       askreddit              0.313944\n",
      "2            iama              0.304565\n",
      "8          videos              0.287835\n",
      "6            pics              0.282841\n",
      "10  todayilearned              0.281916\n"
     ]
    }
   ],
   "source": [
    "# Calculate (In/Out)-Degree centrality (in and out degree)\n",
    "total_degree = in_degree.merge(out_degree, on='SOURCE', suffixes=('_in', '_out'))\n",
    "total_degree['total_degree_centrality'] = total_degree['degree_in'] + total_degree['degree_out']\n",
    "total_degree_centrality = total_degree.sort_values('total_degree_centrality', ascending=False)\n",
    "\n",
    "log(\"Total Degree-centrality:\")\n",
    "print(total_degree.head(5))\n",
    "\n",
    "# Calculate betweenness centrality using an approximation measure for time reasons\n",
    "# If we calculate betweenness centrality for k=n then it will take too much (we can use a sample of k nodes)\n",
    "\n",
    "# TODO: Maybe here calculate multiple betweeness centrality and average the result\n",
    "betweennes_calculation = nx.betweenness_centrality(mGraph, k = 100)\n",
    "betweenness_centrality = pd.DataFrame(betweennes_calculation.items(), columns=['SOURCE', 'betweenness_centrality'])\n",
    "\n",
    "betweenness_centrality = betweenness_centrality.sort_values('betweenness_centrality', ascending=False)\n",
    "\n",
    "log(\"Betweenness Centrality: (Influence of node in graph)\")\n",
    "print(betweenness_centrality.head(5))\n",
    "\n",
    "\n",
    "# Calculate eigenvector centrality. We need a function\n",
    "# which does that for multiGraph instead of singleGraph\n",
    "def m_eigenvector_centrality(G, max_iter=100, tol=1e-6):\n",
    "    nodes = G.nodes()\n",
    "    n = len(nodes)\n",
    "    eigenvector = {node: 1 / n for node in nodes}\n",
    "\n",
    "    # Go until max iterations\n",
    "    for _ in range(max_iter):\n",
    "        new_eigenvector = {}\n",
    "\n",
    "        # Find centrality values\n",
    "        for node in nodes:\n",
    "            centrality = sum(eigenvector[v] for v in G.successors(node))\n",
    "            new_eigenvector[node] = centrality\n",
    "\n",
    "        # Normalize the eigenvector\n",
    "        norm = np.linalg.norm(list(new_eigenvector.values()))\n",
    "        new_eigenvector = {k: v / norm for k, v in new_eigenvector.items()}\n",
    "\n",
    "        # Check for convergence\n",
    "        if sum((new_eigenvector[node] - eigenvector[node])**2 for node in nodes) < tol:\n",
    "            return new_eigenvector\n",
    "\n",
    "        eigenvector = new_eigenvector\n",
    "    return eigenvector\n",
    "\n",
    "eigenvector_calculation = m_eigenvector_centrality(mGraph)\n",
    "eigenvector_centrality = pd.DataFrame(eigenvector_calculation.items(), columns=['SOURCE', 'eigenvector_centrality'])\n",
    "eigenvector_centrality = eigenvector_centrality.sort_values('eigenvector_centrality', ascending=False)\n",
    "\n",
    "log(\"Top 5 Nodes by Eigenvector Centrality:\")\n",
    "print(eigenvector_centrality.head(5))\n",
    "\n",
    "# Use a subset of nodes to calculate closeness centrality for. The subset is chosen as\n",
    "# the combination of other centrality values\n",
    "combined_centrality = total_degree_centrality.merge(betweenness_centrality, on='SOURCE')\n",
    "combined_centrality = combined_centrality.merge(eigenvector_centrality, on='SOURCE')\n",
    "\n",
    "# Just sum the values\n",
    "combined_centrality['combined_centrality'] = (\n",
    "    combined_centrality['total_degree_centrality'] +\n",
    "    combined_centrality['betweenness_centrality'] +\n",
    "    combined_centrality['eigenvector_centrality']\n",
    ")\n",
    "\n",
    "combined_centrality = combined_centrality.sort_values('combined_centrality', ascending=False)\n",
    "# Select the top 20\n",
    "top_nodes = combined_centrality.head(20)\n",
    "top_nodes_list = top_nodes['SOURCE'].tolist()\n",
    "\n",
    "closeness_centrality_calc = {}\n",
    "\n",
    "for node in top_nodes_list:\n",
    "    closeness_centrality_calc[node] = nx.closeness_centrality(mGraph, u=node)\n",
    "closeness_centrality = pd.DataFrame(closeness_centrality_calc.items(), columns=['SOURCE', 'closeness_centrality'])\n",
    "closeness_centrality = closeness_centrality.sort_values('closeness_centrality', ascending=False)\n",
    "\n",
    "log(\"Top 5 Nodes by Closeness Centrality:\")\n",
    "print(closeness_centrality.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation of Clustering Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:22:19] Number of nodes with a clustering coefficient = 1:\n",
      "1498\n"
     ]
    }
   ],
   "source": [
    "def f_clustering_coefficient(G, node):\n",
    "    # List of nodes connected to it\n",
    "    neighbors = list(G.neighbors(node))\n",
    "    l = len(neighbors)\n",
    "    # If there are less than two neighbors then coefficient = 0\n",
    "    if l < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Sum all edges and divide it by all possible edges\n",
    "    possible_edges = l * (l - 1) \n",
    "    actual_edges = sum(1 for v1 in neighbors for v2 in neighbors if G.has_edge(v1, v2))\n",
    "\n",
    "    return actual_edges / possible_edges\n",
    "\n",
    "\n",
    "clustering_coefficient_calc = {}\n",
    "for node in mGraph.nodes():\n",
    "    clustering_coefficient_calc[node] = f_clustering_coefficient(mGraph, node)\n",
    "\n",
    "clustering_coefficient = pd.DataFrame(clustering_coefficient_calc.items(), columns=['SOURCE', 'clustering_coefficient'])\n",
    "count_of_ones = len(clustering_coefficient[clustering_coefficient['clustering_coefficient'] == 1])\n",
    "\n",
    "log(\"Number of nodes with a clustering coefficient = 1:\")\n",
    "print(count_of_ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network Diameter + Number of Connected Components + Size of connected components + Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:39:56] Number of Strongly Connected Components:\n",
      "24071\n",
      "[10:39:56] Top 5 Strongly Connected Components by size:\n",
      "Component 1: Size = 11564 nodes\n",
      "Component 2: Size = 6 nodes\n",
      "Component 3: Size = 5 nodes\n",
      "Component 4: Size = 4 nodes\n",
      "Component 5: Size = 4 nodes\n",
      "[10:39:56] The density of the Graph is\n",
      "0.00022389565198911403\n",
      "[10:39:58] Diameter approximation of largest SCC:\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "strongly_connected_components = list(nx.strongly_connected_components(mGraph))\n",
    "component_sizes = [len(component) for component in strongly_connected_components]\n",
    "sorted_sizes = sorted(component_sizes, reverse=True)\n",
    "\n",
    "log(\"Number of Strongly Connected Components:\")\n",
    "print(len(strongly_connected_components))\n",
    "log(\"Top 5 Strongly Connected Components by size:\")\n",
    "for i, size in enumerate(sorted_sizes[:5], start=1):\n",
    "    print(f\"Component {i}: Size = {size} nodes\")\n",
    "\n",
    "density = nx.density(mGraph)\n",
    "log(\"The density of the Graph is\")\n",
    "print(density)\n",
    "\n",
    "# Calculate diameter only for the biggest SCC\n",
    "largest_scc = max(strongly_connected_components, key=len)\n",
    "largest_scc_subgraph = mGraph.subgraph(largest_scc)\n",
    "# diameter = nx.diameter(largest_scc_subgraph, weight=\"weight\", usebounds=True)\n",
    "diameter = approximation.diameter(largest_scc_subgraph)\n",
    "\n",
    "log(\"Diameter approximation of largest SCC:\")\n",
    "print(diameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Cliques (Honestly I don't think this is necessary since afaik Cliques are defined for undirected graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest Clique 1, Length 34: ['seahawks', 'nfl', 'denverbroncos', 'redskins', 'patriots', 'nyjets', 'eagles', 'saints', 'minnesotavikings', 'oaklandraiders', 'detroitlions', 'losangelesrams', 'tennesseetitans', '49ers', 'nygiants', 'buffalobills', 'jaguars', 'nfl_draft', 'chibears', 'browns', 'buccaneers', 'greenbaypackers', 'texans', 'chargers', 'steelers', 'falcons', 'ravens', 'panthers', 'miamidolphins', 'colts', 'azcardinals', 'cowboys', 'bengals', 'kansascitychiefs']\n",
      "Largest Clique 2, Length 25: ['seahawks', 'nfl', 'stlouisrams', 'chargers', 'ravens', 'colts', 'bengals', 'greenbaypackers', 'chibears', 'redskins', 'texans', 'nyjets', 'detroitlions', 'panthers', 'azcardinals', 'kansascitychiefs', 'nfl_draft', 'losangelesrams', 'tennesseetitans', 'steelers', 'falcons', 'miamidolphins', '49ers', 'cowboys', 'nygiants']\n",
      "Largest Clique 3, Length 24: ['seahawks', 'nfl', 'denverbroncos', 'redskins', 'patriots', 'nyjets', 'eagles', 'saints', 'minnesotavikings', 'oaklandraiders', 'detroitlions', 'fantasyfootball', 'jaguars', 'chibears', 'texans', 'greenbaypackers', 'chargers', 'ravens', 'panthers', 'colts', 'azcardinals', 'browns', 'bengals', 'kansascitychiefs']\n",
      "Largest Clique 4, Length 22: ['donaldglover', 'kanye', 'hiphopcirclejerk', 'travisscott', 'popheadscirclejerk', 'kidcudi', 'asapmob', 'theweeknd', 'octobersveryown', 'frankocean', 'charlieputh', 'kendricklamar', 'runthejewels', 'ofwgkta', 'boogalized', 'drizzy', 'blackhippy', 'prince', 'country', '21savage', 'lilyachty', 'aaliyahdanahaughton']\n",
      "Largest Clique 5, Length 22: ['prince', 'popheadscirclejerk', 'octobersveryown', 'country', 'chancetherapper', 'kidcudi', 'travisscott', 'asapmob', 'theweeknd', 'blackhippy', 'hiphopcirclejerk', 'frankocean', 'charlieputh', 'kendricklamar', 'kanye', 'runthejewels', 'ofwgkta', 'boogalized', '21savage', 'drizzy', 'lilyachty', 'aaliyahdanahaughton']\n"
     ]
    }
   ],
   "source": [
    "# Convert to undirected graph\n",
    "undirected_graph = mGraph.to_undirected()\n",
    "\n",
    "cliques = list(nx.find_cliques(undirected_graph))\n",
    "\n",
    "sorted_cliques = sorted(cliques, key=lambda x: len(x), reverse=True)\n",
    "\n",
    "# Print the 5 largest cliques\n",
    "for i, clique in enumerate(sorted_cliques[:5], start=1):\n",
    "    print(f\"Largest Clique {i}, Length {len(clique)}: {clique}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run homophily test with NSFW and SFW subreddits (not done yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important nodes acting as Bridges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Nodes in Bridge Edges:\n",
      "Node: askreddit, Count: 291\n",
      "Node: writingprompts, Count: 188\n",
      "Node: tipofmypenis, Count: 188\n",
      "Node: iama, Count: 183\n",
      "Node: mhoc, Count: 133\n"
     ]
    }
   ],
   "source": [
    "# We can convert to unidirected simple graph since it will not change the result of edge removal\n",
    "simple_graph = mGraph.to_undirected()\n",
    "\n",
    "# Find bridge edges in the undirected graph\n",
    "bridge_edges = list(nx.bridges(simple_graph))\n",
    "\n",
    "bridge_nodes = [node for edge in bridge_edges for node in edge]\n",
    "\n",
    "# Count the occurrences of each node\n",
    "top_nodes = Counter(bridge_nodes)\n",
    "\n",
    "# Get the top 5 nodes with the highest counts\n",
    "top_nodes = top_nodes.most_common(5)\n",
    "\n",
    "# Print the top 5 nodes\n",
    "print(\"Top 5 Nodes in Bridge Edges:\")\n",
    "for node, count in top_nodes:\n",
    "    print(f\"Node: {node}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Girvan-Newman algorithm (not done yet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
