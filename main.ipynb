{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math as m\n",
    "\n",
    "from utility import log\n",
    "\n",
    "from networkx.algorithms import approximation\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Before starting with the actual algorithms we need to apply some prepocessing. For example removing data that is not relevant for our research, and extracting all nodes and links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:03:32] Loading data\n"
     ]
    }
   ],
   "source": [
    "# Load the datafile\n",
    "log(\"Loading data\")\n",
    "file_path = \"data/soc-redditHyperlinks-body.tsv\"\n",
    "raw_data = pd.read_csv(file_path, sep=\"\\t\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:03:37] Dropping\n",
      "[12:03:37] Creating list of all nodes\n",
      "[12:03:37] Nodes: 35,776\n",
      "[12:03:37] Links: 286,561\n"
     ]
    }
   ],
   "source": [
    "# Remove columns we won't be using\n",
    "log(\"Dropping\")\n",
    "data = raw_data.drop(columns=['POST_ID', 'PROPERTIES'])\n",
    "\n",
    "# Create a list of all nodes\n",
    "log(\"Creating list of all nodes\")\n",
    "nodes = pd.concat([data['SOURCE_SUBREDDIT'], data['TARGET_SUBREDDIT']], axis=0)\n",
    "nodes.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "log(f\"Nodes: {nodes.size:,}\")\n",
    "log(f\"Links: {data.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:03:39] Compressed links: 137,821\n"
     ]
    }
   ],
   "source": [
    "# Reduce the amount of links to just one direct link.\n",
    "# This can be useful if we want a more compressed view of the data.\n",
    "links_compressed = data.drop(columns=[\"TIMESTAMP\"]).groupby(['SOURCE_SUBREDDIT', 'TARGET_SUBREDDIT']).agg('sum')\n",
    "\n",
    "log(f\"Compressed links: {links_compressed.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:03:41] Preparing nx.MultiDiGraph()...\n",
      "[12:03:41] Adding nodes...\n",
      "[12:03:42] Adding edges...\n"
     ]
    }
   ],
   "source": [
    "log(\"Preparing nx.MultiDiGraph()...\")\n",
    "mGraph = nx.MultiDiGraph()\n",
    "\n",
    "log(\"Adding nodes...\")\n",
    "for node in nodes:\n",
    "    mGraph.add_node(node)\n",
    "\n",
    "log(\"Adding edges...\")\n",
    "for index, edge in data.iterrows():\n",
    "    mGraph.add_edge(edge['SOURCE_SUBREDDIT'], edge['TARGET_SUBREDDIT'], sentiment=edge['LINK_SENTIMENT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anaylsis\n",
    "This is the analysis of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:22] mGraph > Vertices: \t35,776\n",
      "[12:05:22] mGraph > Edges: \t286,561\n",
      "[12:05:22] In degree: (Most referenced subreddits)\n",
      "               SOURCE  degree\n",
      "34          askreddit    7329\n",
      "22               iama    3694\n",
      "7741             pics    2779\n",
      "42     writingprompts    2490\n",
      "11294          videos    2446\n",
      "[12:05:22] Out degree: (Referencing other subreddits)\n",
      "               SOURCE  degree\n",
      "70     subredditdrama    4665\n",
      "16        circlebroke    2358\n",
      "6400  shitliberalssay    1968\n",
      "126      outoftheloop    1958\n",
      "79          copypasta    1824\n"
     ]
    }
   ],
   "source": [
    "log(f\"mGraph > Vertices: \\t{mGraph.number_of_nodes():,}\")\n",
    "log(f\"mGraph > Edges: \\t{mGraph.number_of_edges():,}\")\n",
    "\n",
    "in_degree = pd.DataFrame(mGraph.in_degree(), columns=['SOURCE', 'degree'])\n",
    "out_degree = pd.DataFrame(mGraph.out_degree(), columns=['SOURCE', 'degree'])\n",
    "\n",
    "in_degree.sort_values('degree', ascending=False, inplace=True)\n",
    "out_degree.sort_values('degree', ascending=False, inplace=True)\n",
    "\n",
    "log(\"In degree: (Most referenced subreddits)\")\n",
    "print(in_degree.head(5))\n",
    "\n",
    "log(\"Out degree: (Referencing other subreddits)\")\n",
    "print(out_degree.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation of centrality indices: Degree Centrality, Betweenness Centrality, Closeness Centrality and Eigenvector Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:08:35] Total Degree-centrality:\n",
      "           SOURCE  degree_in  degree_out  total_degree_centrality\n",
      "0       askreddit       7329        1338                     8667\n",
      "1            iama       3694        1181                     4875\n",
      "2            pics       2779           4                     2783\n",
      "3  writingprompts       2490        1707                     4197\n",
      "4          videos       2446           4                     2450\n",
      "[12:08:45] Betweenness Centrality: (Influence of node in graph)\n",
      "             SOURCE  betweenness_centrality\n",
      "22             iama                0.050840\n",
      "34        askreddit                0.046842\n",
      "70   subredditdrama                0.046078\n",
      "126    outoftheloop                0.042275\n",
      "42   writingprompts                0.019658\n",
      "Total Nodes after Pruning: 314\n",
      "Total Edges after Pruning: 56691\n",
      "[12:08:46] Top 5 Nodes by Eigenvector Centrality:\n",
      "             SOURCE  eigenvector_centrality\n",
      "51         buildapc                0.690689\n",
      "131     techsupport                0.603486\n",
      "163   buildapcforme                0.254779\n",
      "293       windows10                0.135070\n",
      "24   subredditdrama                0.101864\n",
      "[12:09:19] Top 5 Nodes by Closeness Centrality:\n",
      "           SOURCE  closeness_centrality\n",
      "0       askreddit              0.313944\n",
      "2            iama              0.304565\n",
      "8          videos              0.287835\n",
      "6            pics              0.282841\n",
      "10  todayilearned              0.281916\n"
     ]
    }
   ],
   "source": [
    "# Calculate (In/Out)-Degree centrality (in and out degree)\n",
    "total_degree = in_degree.merge(out_degree, on='SOURCE', suffixes=('_in', '_out'))\n",
    "total_degree['total_degree_centrality'] = total_degree['degree_in'] + total_degree['degree_out']\n",
    "total_degree_centrality = total_degree.sort_values('total_degree_centrality', ascending=False)\n",
    "\n",
    "\n",
    "log(\"Total Degree-centrality:\")\n",
    "print(total_degree.head(5))\n",
    "\n",
    "\n",
    "# Prune low connectivity nodes to speed up the computation of certain properties of the graph\n",
    "def prune_nodes(G):\n",
    "    threshold = 300\n",
    "    graph_copy = G.copy()\n",
    "    nodes_to_remove = total_degree_centrality[total_degree_centrality['total_degree_centrality'] < threshold]['SOURCE'].tolist()\n",
    "    graph_copy.remove_nodes_from(nodes_to_remove)\n",
    "    total_nodes = graph_copy.number_of_nodes()\n",
    "    total_edges = graph_copy.number_of_edges()\n",
    "    print(f\"Total Nodes after Pruning: {total_nodes}\")\n",
    "    print(f\"Total Edges after Pruning: {total_edges}\")\n",
    "    return graph_copy\n",
    "\n",
    "# Calculate betweenness centrality using an approximation measure for time reasons\n",
    "# If we calculate betweenness centrality for k=n then it will take too much (we can use a sample of k nodes)\n",
    "\n",
    "betweennes_calculation = nx.betweenness_centrality(mGraph, k = 100)\n",
    "betweenness_centrality = pd.DataFrame(betweennes_calculation.items(), columns=['SOURCE', 'betweenness_centrality'])\n",
    "\n",
    "betweenness_centrality = betweenness_centrality.sort_values('betweenness_centrality', ascending=False)\n",
    "\n",
    "log(\"Betweenness Centrality: (Influence of node in graph)\")\n",
    "print(betweenness_centrality.head(5))\n",
    "\n",
    "\n",
    "# Calculate eigenvector centrality. We need a function\n",
    "# which does that for multiGraph instead of singleGraph\n",
    "# Used the same start parameters (mat_iter and tol) as NetworkX\n",
    "def m_eigenvector_centrality(G, max_iter=100, tol=1e-6):\n",
    "    # Prune some nodes of the matrix otherwise my PC crashes!\n",
    "    G_copy = prune_nodes(G)\n",
    "    # Make an adjacency matrix\n",
    "    adjacency_matrix = csr_matrix(nx.adjacency_matrix(G_copy))\n",
    "\n",
    "    # Array to save eigenvector centrality for each node\n",
    "    num_nodes = len(G_copy.nodes())\n",
    "    eigenvector = np.full(num_nodes, 1.0 / num_nodes)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # Calculate the matrix-vector product\n",
    "        new_eigenvector = np.dot(adjacency_matrix.toarray(), eigenvector)\n",
    "\n",
    "        # Normalize\n",
    "        norm = np.linalg.norm(new_eigenvector)\n",
    "        new_eigenvector = new_eigenvector / norm\n",
    "\n",
    "        # Repeat until convergence (or max_iter)\n",
    "        if np.linalg.norm(new_eigenvector - eigenvector) < tol:\n",
    "            return dict(zip(G_copy.nodes(), new_eigenvector))\n",
    "\n",
    "        eigenvector = new_eigenvector\n",
    "\n",
    "    return dict(zip(G_copy.nodes(), eigenvector))\n",
    "\n",
    "eigenvector_calculation = m_eigenvector_centrality(mGraph)\n",
    "eigenvector_centrality = pd.DataFrame(eigenvector_calculation.items(), columns=['SOURCE', 'eigenvector_centrality'])\n",
    "eigenvector_centrality = eigenvector_centrality.sort_values('eigenvector_centrality', ascending=False)\n",
    "\n",
    "log(\"Top 5 Nodes by Eigenvector Centrality:\")\n",
    "print(eigenvector_centrality.head(5))\n",
    "\n",
    "# Use a subset of nodes to calculate closeness centrality for. The subset is chosen as\n",
    "# the combination of other centrality values\n",
    "combined_centrality = total_degree_centrality.merge(betweenness_centrality, on='SOURCE')\n",
    "combined_centrality = combined_centrality.merge(eigenvector_centrality, on='SOURCE')\n",
    "\n",
    "# Just sum the values\n",
    "combined_centrality['combined_centrality'] = (\n",
    "    combined_centrality['total_degree_centrality'] +\n",
    "    combined_centrality['betweenness_centrality'] +\n",
    "    combined_centrality['eigenvector_centrality']\n",
    ")\n",
    "\n",
    "combined_centrality = combined_centrality.sort_values('combined_centrality', ascending=False)\n",
    "# Select the top 20\n",
    "top_nodes = combined_centrality.head(20)\n",
    "top_nodes_list = top_nodes['SOURCE'].tolist()\n",
    "\n",
    "closeness_centrality_calc = {}\n",
    "\n",
    "for node in top_nodes_list:\n",
    "    closeness_centrality_calc[node] = nx.closeness_centrality(mGraph, u=node)\n",
    "closeness_centrality = pd.DataFrame(closeness_centrality_calc.items(), columns=['SOURCE', 'closeness_centrality'])\n",
    "closeness_centrality = closeness_centrality.sort_values('closeness_centrality', ascending=False)\n",
    "\n",
    "log(\"Top 5 Nodes by Closeness Centrality:\")\n",
    "print(closeness_centrality.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation of Clustering Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:01:43] Number of nodes with a clustering coefficient = 1:\n",
      "1498\n"
     ]
    }
   ],
   "source": [
    "# As before, we need to make our own function since\n",
    "# networkX does not work for MultiDiGraphs\n",
    "def f_clustering_coefficient(G, node):\n",
    "    # Get the neighbors of the given node\n",
    "    neighbors = set(G.neighbors(node))\n",
    "\n",
    "    # If there are no neighbors then return 0\n",
    "    if len(neighbors) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    # Find the number of edges between neighbors\n",
    "    edges_between_neighbors = 0\n",
    "\n",
    "    for u in neighbors:\n",
    "        for v in neighbors:\n",
    "            if u != v and G.has_edge(u, v):\n",
    "                # Usually edge weights are not considered, hence we just add +1\n",
    "                edges_between_neighbors += 1\n",
    "\n",
    "    # Calculate the clustering coefficient\n",
    "    clustering_coefficient = edges_between_neighbors / (len(neighbors) * (len(neighbors) - 1))\n",
    "    return clustering_coefficient\n",
    "\n",
    "\n",
    "clustering_coefficient_calc = {}\n",
    "for node in mGraph.nodes():\n",
    "    clustering_coefficient_calc[node] = f_clustering_coefficient(mGraph, node)\n",
    "\n",
    "clustering_coefficient = pd.DataFrame(clustering_coefficient_calc.items(), columns=['SOURCE', 'clustering_coefficient'])\n",
    "count_of_ones = len(clustering_coefficient[clustering_coefficient['clustering_coefficient'] == 1])\n",
    "\n",
    "# 1 is the highest clustering coefficient value\n",
    "log(\"Number of nodes with a clustering coefficient = 1:\")\n",
    "print(count_of_ones)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network Diameter + Number of Connected Components + Size of connected components + Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:07:58] Number of Strongly Connected Components:\n",
      "24071\n",
      "[11:07:58] Top 5 Strongly Connected Components by size:\n",
      "Component 1: Size = 11564 nodes\n",
      "Component 2: Size = 6 nodes\n",
      "Component 3: Size = 5 nodes\n",
      "Component 4: Size = 4 nodes\n",
      "Component 5: Size = 4 nodes\n",
      "[11:07:58] The density of the Graph is\n",
      "0.00022389565198911403\n",
      "[11:08:02] Diameter approximation of largest SCC:\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "strongly_connected_components = list(nx.strongly_connected_components(mGraph))\n",
    "component_sizes = [len(component) for component in strongly_connected_components]\n",
    "sorted_sizes = sorted(component_sizes, reverse=True)\n",
    "\n",
    "log(\"Number of Strongly Connected Components:\")\n",
    "total_Scc = len(strongly_connected_components)\n",
    "print(total_Scc)\n",
    "log(\"Top 5 Strongly Connected Components by size:\")\n",
    "for i, size in enumerate(sorted_sizes[:5], start=1):\n",
    "    print(f\"Component {i}: Size = {size} nodes\")\n",
    "\n",
    "density = nx.density(mGraph)\n",
    "log(\"The density of the Graph is\")\n",
    "print(density)\n",
    "\n",
    "# Calculate diameter only for the biggest SCC\n",
    "largest_scc = max(strongly_connected_components, key=len)\n",
    "largest_scc_subgraph = mGraph.subgraph(largest_scc)\n",
    "# diameter = nx.diameter(largest_scc_subgraph, weight=\"weight\", usebounds=True)\n",
    "diameter = approximation.diameter(largest_scc_subgraph)\n",
    "\n",
    "log(\"Diameter approximation of largest SCC:\")\n",
    "print(diameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Cliques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest Clique 1, Length 33: ['greenbaypackers', 'seahawks', '49ers', 'ravens', 'azcardinals', 'nfl', 'panthers', 'oaklandraiders', 'texans', 'bengals', 'colts', 'steelers', 'eagles', 'buffalobills', 'chibears', 'nfl_draft', 'falcons', 'browns', 'denverbroncos', 'cowboys', 'kansascitychiefs', 'saints', 'jaguars', 'nyjets', 'chargers', 'redskins', 'detroitlions', 'buccaneers', 'miamidolphins', 'nygiants', 'minnesotavikings', 'losangelesrams', 'patriots']\n",
      "Largest Clique 2, Length 33: ['greenbaypackers', 'seahawks', '49ers', 'ravens', 'azcardinals', 'nfl', 'panthers', 'oaklandraiders', 'texans', 'bengals', 'colts', 'steelers', 'eagles', 'buffalobills', 'chibears', 'nfl_draft', 'falcons', 'browns', 'denverbroncos', 'cowboys', 'kansascitychiefs', 'saints', 'jaguars', 'nyjets', 'chargers', 'redskins', 'detroitlions', 'buccaneers', 'miamidolphins', 'nygiants', 'minnesotavikings', 'losangelesrams', 'tennesseetitans']\n",
      "Largest Clique 3, Length 10: ['mbotandgtron', 'nomina', 'cqb', 'pears', 'killlakill', 'iupui', 'thewire', 'nvidiashield', 'pmmodi', 'vmware']\n",
      "Largest Clique 4, Length 10: ['raisedbynarcissists', 'suicidewatch', 'legaladvice', 'depression', 'anxiety', 'relationship_advice', 'self', 'offmychest', 'relationships', 'advice']\n",
      "Largest Clique 5, Length 9: ['raisedbynarcissists', 'askreddit', 'offmychest', 'relationships', 'advice', 'relationship_advice', 'legaladvice', 'self', 'anxiety']\n"
     ]
    }
   ],
   "source": [
    "# Convert to undirected graph. Explanation to why this is fine (https://stackoverflow.com/questions/12896477/cliques-for-directed-graphs-in-igraph)\n",
    "undirected_graph = nx.Graph()\n",
    "\n",
    "# Add an edge (u,v) in the undirected graph iff in our multidigraph we have\n",
    "# (u,v) and (v,u)\n",
    "for u, v in mGraph.edges():\n",
    "    # Check if there is an edge (v, u) in the MultiDiGraph\n",
    "    if mGraph.has_edge(v, u):\n",
    "        undirected_graph.add_edge(u, v)\n",
    "\n",
    "cliques = list(nx.find_cliques(undirected_graph))\n",
    "\n",
    "sorted_cliques = sorted(cliques, key=lambda x: len(x), reverse=True)\n",
    "\n",
    "# Print the 5 largest cliques\n",
    "for i, clique in enumerate(sorted_cliques[:5], start=1):\n",
    "    print(f\"Largest Clique {i}, Length {len(clique)}: {clique}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run homophily test with NSFW and SFW subreddits (not done yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important nodes acting as Bridges. Calculation for MultiDiGraph (Paper: https://core.ac.uk/reader/82453490) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Nodes after Pruning: 314\n",
      "Total Edges after Pruning: 56691\n",
      "Processed 0 edges\n",
      "Processed 5000 edges\n",
      "Processed 10000 edges\n",
      "Processed 15000 edges\n",
      "Processed 20000 edges\n",
      "Processed 25000 edges\n",
      "Processed 30000 edges\n",
      "Processed 35000 edges\n",
      "Processed 40000 edges\n",
      "Processed 45000 edges\n",
      "Processed 50000 edges\n",
      "Processed 55000 edges\n",
      "[('footballhighlights', 'soccer'), ('gifs', 'resissues'), ('pics', 'changelog'), ('ironthronepowers', 'bugs'), ('modelusgov', 'mhoc')]\n",
      "Top 5 Nodes in Bridge Edges:\n",
      "Node: footballhighlights, Count: 1\n",
      "Node: soccer, Count: 1\n",
      "Node: gifs, Count: 1\n",
      "Node: resissues, Count: 1\n",
      "Node: pics, Count: 1\n"
     ]
    }
   ],
   "source": [
    "# We say that a bridge in a Graph is a strong bridge if removing it\n",
    "# increases the number of SCC (ref: paper above)\n",
    "\n",
    "def find_bridges(G):\n",
    "    bridges = []\n",
    "\n",
    "    total_scc = len(list(nx.strongly_connected_components(G)))\n",
    "    graph_copy = G.copy()\n",
    "    count = 0\n",
    "    for u, v in G.edges():\n",
    "        if count % 5000 == 0:\n",
    "            print(f\"Processed {count} edges\")\n",
    "        count += 1\n",
    "        # Temporarily remove the edge (u, v)\n",
    "        graph_copy.remove_edge(u, v)\n",
    "        new_scc_count = len(list(nx.strongly_connected_components(graph_copy)))\n",
    "\n",
    "        # If the number of new SCC < original SCC then the edge is bridge\n",
    "        if new_scc_count > total_scc:\n",
    "            bridges.append((u, v))\n",
    "\n",
    "        # Restore edge (u, v)\n",
    "        graph_copy.add_edge(u, v)\n",
    "    return bridges\n",
    "\n",
    "# Find bridge edges in the undirected graph\n",
    "bridge_edges = list(find_bridges(prune_nodes(mGraph)))\n",
    "print(bridge_edges)\n",
    "bridge_nodes = [node for edge in bridge_edges for node in edge]\n",
    "\n",
    "# Count the occurrences of each node\n",
    "top_nodes = Counter(bridge_nodes)\n",
    "\n",
    "# Get the top 5 nodes with the highest counts\n",
    "top_nodes = top_nodes.most_common(5)\n",
    "\n",
    "# Print the top 5 nodes\n",
    "print(\"Top 5 Nodes in Bridge Edges:\")\n",
    "for node, count in top_nodes:\n",
    "    print(f\"Node: {node}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important nodes acting as Bridges. Calculation for Undirected graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Nodes in Bridge Edges (undirected graphs):\n",
      "Node: askreddit, Count: 291\n",
      "Node: writingprompts, Count: 188\n",
      "Node: tipofmypenis, Count: 188\n",
      "Node: iama, Count: 183\n",
      "Node: mhoc, Count: 133\n"
     ]
    }
   ],
   "source": [
    "simple_graph = mGraph.to_undirected()\n",
    "\n",
    "# Find bridge edges in the undirected graph\n",
    "bridge_edges = list(nx.bridges(simple_graph))\n",
    "\n",
    "bridge_nodes = [node for edge in bridge_edges for node in edge]\n",
    "\n",
    "# Count the occurrences of each node\n",
    "top_nodes = Counter(bridge_nodes)\n",
    "\n",
    "# Get the top 5 nodes with the highest counts\n",
    "top_nodes = top_nodes.most_common(5)\n",
    "\n",
    "# Print the top 5 nodes\n",
    "print(\"Top 5 Nodes in Bridge Edges (undirected graphs):\")\n",
    "for node, count in top_nodes:\n",
    "    print(f\"Node: {node}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Girvan-Newman algorithm (not done yet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
