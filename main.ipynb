{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math as m\n",
    "\n",
    "from utility import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Before starting with the actual algorithms we need to apply some prepocessing. For example removing data that is not relevant for our research, and extracting all nodes and links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:59:34] Loading data\n"
     ]
    }
   ],
   "source": [
    "# Load the datafile\n",
    "log(\"Loading data\")\n",
    "file_path = \"data\\soc-redditHyperlinks-body.tsv\"\n",
    "raw_data = pd.read_csv(file_path, sep=\"\\t\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:59:50] Dropping\n",
      "[09:59:50] Creating list of all nodes\n",
      "[09:59:50] Nodes: 35,776\n",
      "[09:59:50] Links: 286,561\n"
     ]
    }
   ],
   "source": [
    "# Remove columns we won't be using\n",
    "log(\"Dropping\")\n",
    "data = raw_data.drop(columns=['POST_ID', 'PROPERTIES'])\n",
    "\n",
    "# Create a list of all nodes\n",
    "log(\"Creating list of all nodes\")\n",
    "nodes = pd.concat([data['SOURCE_SUBREDDIT'], data['TARGET_SUBREDDIT']], axis=0)\n",
    "nodes.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "log(f\"Nodes: {nodes.size:,}\")\n",
    "log(f\"Links: {data.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:06:25] Compressed links: 137,821\n"
     ]
    }
   ],
   "source": [
    "# Reduce the amount of links to just one direct link.\n",
    "# This can be useful if we want a more compressed view of the data.\n",
    "links_compressed = data.drop(columns=[\"TIMESTAMP\"]).groupby(['SOURCE_SUBREDDIT', 'TARGET_SUBREDDIT']).agg('sum')\n",
    "\n",
    "log(f\"Compressed links: {links_compressed.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:17:25] Preparing nx.MultiDiGraph()...\n",
      "[10:17:25] Adding nodes...\n",
      "[10:17:25] Adding edges...\n"
     ]
    }
   ],
   "source": [
    "log(\"Preparing nx.MultiDiGraph()...\")\n",
    "mGraph = nx.MultiDiGraph()\n",
    "\n",
    "log(\"Adding nodes...\")\n",
    "for node in nodes:\n",
    "    mGraph.add_node(node)\n",
    "\n",
    "log(\"Adding edges...\")\n",
    "for index, edge in data.iterrows():\n",
    "    mGraph.add_edge(edge['SOURCE_SUBREDDIT'], edge['TARGET_SUBREDDIT'], sentiment=edge['LINK_SENTIMENT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anaylsis\n",
    "This is the analysis of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:54:00] mGraph > Vertices: \t35,776\n",
      "[10:54:00] mGraph > Edges: \t286,561\n",
      "[10:54:00] In degree: (Most referenced subreddits)\n",
      "               SOURCE  degree\n",
      "34          askreddit    7329\n",
      "22               iama    3694\n",
      "7741             pics    2779\n",
      "42     writingprompts    2490\n",
      "11294          videos    2446\n",
      "[10:54:00] Out degree: (Referencing other subreddits)\n",
      "               SOURCE  degree\n",
      "70     subredditdrama    4665\n",
      "16        circlebroke    2358\n",
      "6400  shitliberalssay    1968\n",
      "126      outoftheloop    1958\n",
      "79          copypasta    1824\n"
     ]
    }
   ],
   "source": [
    "log(f\"mGraph > Vertices: \\t{mGraph.number_of_nodes():,}\")\n",
    "log(f\"mGraph > Edges: \\t{mGraph.number_of_edges():,}\")\n",
    "\n",
    "in_degree = pd.DataFrame(mGraph.in_degree(), columns=['SOURCE', 'degree'])\n",
    "out_degree = pd.DataFrame(mGraph.out_degree(), columns=['SOURCE', 'degree'])\n",
    "\n",
    "in_degree.sort_values('degree', ascending=False, inplace=True)\n",
    "out_degree.sort_values('degree', ascending=False, inplace=True)\n",
    "\n",
    "log(\"In degree: (Most referenced subreddits)\")\n",
    "print(in_degree.head(5))\n",
    "\n",
    "log(\"Out degree: (Referencing other subreddits)\")\n",
    "print(out_degree.head(5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
