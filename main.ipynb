{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math as m\n",
    "\n",
    "from utility import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Before starting with the actual algorithms we need to apply some prepocessing. For example removing data that is not relevant for our research, and extracting all nodes and links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:32:32] Loading data\n"
     ]
    }
   ],
   "source": [
    "# Load the datafile\n",
    "log(\"Loading data\")\n",
    "file_path = \"data/soc-redditHyperlinks-body.tsv\"\n",
    "raw_data = pd.read_csv(file_path, sep=\"\\t\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:33:26] Dropping\n",
      "[14:33:26] Creating list of all nodes\n",
      "[14:33:26] Nodes: 35,776\n",
      "[14:33:26] Links: 286,561\n"
     ]
    }
   ],
   "source": [
    "# Remove columns we won't be using\n",
    "log(\"Dropping\")\n",
    "data = raw_data.drop(columns=['POST_ID', 'PROPERTIES'])\n",
    "\n",
    "# Create a list of all nodes\n",
    "log(\"Creating list of all nodes\")\n",
    "nodes = pd.concat([data['SOURCE_SUBREDDIT'], data['TARGET_SUBREDDIT']], axis=0)\n",
    "nodes.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "log(f\"Nodes: {nodes.size:,}\")\n",
    "log(f\"Links: {data.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:33:29] Compressed links: 137,821\n"
     ]
    }
   ],
   "source": [
    "# Reduce the amount of links to just one direct link.\n",
    "# This can be useful if we want a more compressed view of the data.\n",
    "links_compressed = data.drop(columns=[\"TIMESTAMP\"]).groupby(['SOURCE_SUBREDDIT', 'TARGET_SUBREDDIT']).agg('sum')\n",
    "\n",
    "log(f\"Compressed links: {links_compressed.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:33:34] Preparing nx.MultiDiGraph()...\n",
      "[14:33:34] Adding nodes...\n",
      "[14:33:34] Adding edges...\n"
     ]
    }
   ],
   "source": [
    "log(\"Preparing nx.MultiDiGraph()...\")\n",
    "mGraph = nx.MultiDiGraph()\n",
    "\n",
    "log(\"Adding nodes...\")\n",
    "for node in nodes:\n",
    "    mGraph.add_node(node)\n",
    "\n",
    "log(\"Adding edges...\")\n",
    "for index, edge in data.iterrows():\n",
    "    mGraph.add_edge(edge['SOURCE_SUBREDDIT'], edge['TARGET_SUBREDDIT'], sentiment=edge['LINK_SENTIMENT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anaylsis\n",
    "This is the analysis of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:10:10] mGraph > Vertices: \t35,776\n",
      "[16:10:10] mGraph > Edges: \t286,561\n",
      "[16:10:10] In degree: (Most referenced subreddits)\n",
      "               SOURCE  degree\n",
      "34          askreddit    7329\n",
      "22               iama    3694\n",
      "7741             pics    2779\n",
      "42     writingprompts    2490\n",
      "11294          videos    2446\n",
      "[16:10:10] Out degree: (Referencing other subreddits)\n",
      "               SOURCE  degree\n",
      "70     subredditdrama    4665\n",
      "16        circlebroke    2358\n",
      "6400  shitliberalssay    1968\n",
      "126      outoftheloop    1958\n",
      "79          copypasta    1824\n",
      "[16:10:10] Total Degree-centrality:\n",
      "           SOURCE  degree_in  degree_out  total_degree_centrality\n",
      "0       askreddit       7329        1338                     8667\n",
      "1            iama       3694        1181                     4875\n",
      "2            pics       2779           4                     2783\n",
      "3  writingprompts       2490        1707                     4197\n",
      "4          videos       2446           4                     2450\n",
      "[16:10:25] Betweenness Centrality: (Influence of node in graph)\n",
      "             SOURCE  betweenness_centrality\n",
      "34        askreddit                0.061469\n",
      "70   subredditdrama                0.049041\n",
      "22             iama                0.048145\n",
      "126    outoftheloop                0.026312\n",
      "42   writingprompts                0.023972\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_3593728/1714094102.py\", line 47, in <module>\n",
      "    closeness_calculation[node] = closeness_centrality_function(mGraph, node)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3593728/1714094102.py\", line 39, in closeness_centrality_function\n",
      "    shortest_paths = nx.single_source_shortest_path_length(G, node)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gebruiker/.local/lib/python3.11/site-packages/networkx/classes/backends.py\", line 148, in wrapper\n",
      "    return func(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gebruiker/.local/lib/python3.11/site-packages/networkx/algorithms/shortest_paths/unweighted.py\", line 62, in single_source_shortest_path_length\n",
      "    return dict(_single_shortest_path_length(G._adj, nextlevel, cutoff))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gebruiker/.local/lib/python3.11/site-packages/networkx/algorithms/shortest_paths/unweighted.py\", line -1, in _single_shortest_path_length\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 1997, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/ultratb.py\", line 1112, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/ultratb.py\", line 1006, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/ultratb.py\", line 859, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/ultratb.py\", line 812, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/ultratb.py\", line 730, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "                                              ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/executing/executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "log(f\"mGraph > Vertices: \\t{mGraph.number_of_nodes():,}\")\n",
    "log(f\"mGraph > Edges: \\t{mGraph.number_of_edges():,}\")\n",
    "\n",
    "in_degree = pd.DataFrame(mGraph.in_degree(), columns=['SOURCE', 'degree'])\n",
    "out_degree = pd.DataFrame(mGraph.out_degree(), columns=['SOURCE', 'degree'])\n",
    "\n",
    "in_degree.sort_values('degree', ascending=False, inplace=True)\n",
    "out_degree.sort_values('degree', ascending=False, inplace=True)\n",
    "\n",
    "log(\"In degree: (Most referenced subreddits)\")\n",
    "print(in_degree.head(5))\n",
    "\n",
    "log(\"Out degree: (Referencing other subreddits)\")\n",
    "print(out_degree.head(5))\n",
    "\n",
    "# Calculate (In/Out)-Degree centrality (in and out degree)\n",
    "total_degree = in_degree.merge(out_degree, on='SOURCE', suffixes=('_in', '_out'))\n",
    "total_degree['total_degree_centrality'] = total_degree['degree_in'] + total_degree['degree_out']\n",
    "total_degree_centrality = total_degree.sort_values('total_degree_centrality', ascending=False)\n",
    "\n",
    "log(\"Total Degree-centrality:\")\n",
    "print(total_degree.head(5))\n",
    "\n",
    "# Calculate betweenness centrality using an approximation measure for time reasons\n",
    "# If we calculate betweenness centrality for k=n then it will take too much (we can use a sample of k nodes)\n",
    "\n",
    "# TODO: Maybe here calculate multiple betweeness centrality and average the result\n",
    "betweennes_calculation = nx.betweenness_centrality(mGraph, k = 100)\n",
    "betweenness_centrality = pd.DataFrame(betweennes_calculation.items(), columns=['SOURCE', 'betweenness_centrality'])\n",
    "\n",
    "betweenness_centrality = betweenness_centrality.sort_values('betweenness_centrality', ascending=False)\n",
    "\n",
    "log(\"Betweenness Centrality: (Influence of node in graph)\")\n",
    "print(betweenness_centrality.head(5))\n",
    "\n",
    "# TODO: Calculate closeness centrality: This takes too LONG\n",
    "# closeness_calculation = nx.closeness_centrality(mGraph)\n",
    "# closeness_centrality = pd.DataFrame(closeness_calculation.items(), columns=['SOURCE', 'closeness_centrality'])\n",
    "# closeness_centrality = closeness_centrality.sort_values('closeness_centrality', ascending=False)\n",
    "\n",
    "# print(\"Closeness Centrality:\")\n",
    "# print(closeness_centrality.head(5))\n",
    "\n",
    "# Calculate eigenvector centrality. We need a function\n",
    "# which does that for multiGraph instead of singleGraph\n",
    "def m_eigenvector_centrality(G, max_iter=100, tol=1e-6):\n",
    "    nodes = G.nodes()\n",
    "    n = len(nodes)\n",
    "    eigenvector = {node: 1 / n for node in nodes}\n",
    "\n",
    "    # Go until max iterations\n",
    "    for _ in range(max_iter):\n",
    "        new_eigenvector = {}\n",
    "\n",
    "        # Find centrality values\n",
    "        for node in nodes:\n",
    "            centrality = sum(eigenvector[v] for v in G.successors(node))\n",
    "            new_eigenvector[node] = centrality\n",
    "\n",
    "        # Normalize the eigenvector\n",
    "        norm = np.linalg.norm(list(new_eigenvector.values()))\n",
    "        new_eigenvector = {k: v / norm for k, v in new_eigenvector.items()}\n",
    "\n",
    "        # Check for convergence\n",
    "        if sum((new_eigenvector[node] - eigenvector[node])**2 for node in nodes) < tol:\n",
    "            return new_eigenvector\n",
    "\n",
    "        eigenvector = new_eigenvector\n",
    "    return eigenvector\n",
    "\n",
    "eigenvector_calculation = m_eigenvector_centrality(mGraph)\n",
    "eigenvector_centrality = pd.DataFrame(eigenvector_calculation.items(), columns=['SOURCE', 'eigenvector_centrality'])\n",
    "eigenvector_centrality = eigenvector_centrality.sort_values('eigenvector_centrality', ascending=False)\n",
    "\n",
    "log(\"Top 5 Nodes by Eigenvector Centrality:\")\n",
    "print(eigenvector_centrality.head(5))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
